{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkoQbWSRlTVXk2FLkNMfYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/levietduc1007/machine-learning-regression-exercises/blob/main/PINNs_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Framework cho tất cả các dạng toán\n",
        "Ký hiệu:\n",
        "*   x: input\n",
        "*   u: output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XuB_JgYylvT1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apKwN0LXrVam"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "_IO4wV-7ZHIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Đề: Linear regression"
      ],
      "metadata": {
        "id": "_oYVA0iYsEBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = [[1.0], [2.0], [3.0], [4.0], [5.0]]\n",
        "y_train = [[3.0], [5.0], [7.0], [9.0], [11.0]]"
      ],
      "metadata": {
        "id": "B-hD7H9ysDfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xây dựng Neural"
      ],
      "metadata": {
        "id": "vKjT5kjnrlia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PhysicsInformedNN(nn.Module):\n",
        "  # ép dữ liệu về kiểu pytorch\n",
        "    def __init__(self, input_size, output_size, hidden_size=[20, 20, 5], activation='tanh'):\n",
        "        super(PhysicsInformedNN, self).__init__()\n",
        "        layers_list = []\n",
        "\n",
        "        # Nhận dạng hàm activation\n",
        "        for i in range(len(hidden_size)):\n",
        "            layers_list.append(nn.Linear(self.layers_list[i], self.layers_list[i+1])) #input layer i là output layer i+1\n",
        "\n",
        "            # Không thêm hàm kích hoạt sau lớp cuối cùng\n",
        "            if i < len(hidden_size) - 1:\n",
        "                if activation == 'relu':\n",
        "                    layers_list.append(nn.ReLU())\n",
        "                elif activation == 'sigmoid':\n",
        "                    layers_list.append(nn.Sigmoid())\n",
        "                elif activation == 'tanh':\n",
        "                    layers_list.append(nn.Tanh())\n",
        "                else:\n",
        "                    layers.append(nn.Softplus())\n",
        "\n",
        "        # Khởi tạo Xavier (glorot_normal) cho các lớp Linear\n",
        "        for layer in layers_list:\n",
        "            if isinstance(layer, nn.Linear): # chỉ lọc hàm linear\n",
        "                nn.init.xavier_normal_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "        return nn.Sequential(*layers_list) # Không hiểu cái này là gì, đọc rồi vẫn không hiểu, chỉ biết là cần tại chỗ này\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0 # Chuẩn hóa input để từ -1 đến 1, để tránh ảnh hưởng bởi tham số lớn\n",
        "        return self.net(H) # self.net chứa NN đã được tạo từ nn.Sequential\n",
        "\n",
        "##################### Dự đoán u v và đạo hàm output theo input ###########################################\n",
        "\n",
        "    def get_uv_and_derivatives(self, x):\n",
        "        # Báo cho PyTorch theo dõi đạo hàm của input\n",
        "        x.requires_grad_(True)\n",
        "\n",
        "        x = torch.cat([x, t], dim=1) # gộp lại thành matrix input duy nhất\n",
        "        u_pred = self(x) # chạy NN ra dự đoán u_pred\n",
        "\n",
        "        # Tách rời thành từng tensor output\n",
        "        u = uv[:, 0:1] # gần giống uv[:, 0], khác ở chỗ uv[:, 0:1] lấy cột ra nhưng vẫn giữ nguyên vỏ bọc 2 chiều.\n",
        "        # Ex: [[0.5], [0.8], [0.1]] thay vì [0.5, 0.8, 0.1]\n",
        "\n",
        "        # Yêu cầu để tính đạo hàm cho đầu ra đa chiều\n",
        "        grad_outputs = torch.ones_like(u, device=device) # tạo matrix kích thước giống u với full 1\n",
        "\n",
        "        # Tính đạo hàm bậc 1\n",
        "        # create_graph=True là cực kỳ quan trọng, nó cho phép tính đạo hàm bậc 2\n",
        "        #u_t = torch.autograd.grad(u, t, grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "        #v_t = torch.autograd.grad(v, t, grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "        u_x = torch.autograd.grad(u, x, grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "        #v_x = torch.autograd.grad(v, x, grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "\n",
        "        return u, u_x\n",
        "\n",
        "####################### Tính toán residual của PDE (f_u, f_v)##################################\n",
        "\n",
        "    def get_f_uv(self, x, t):\n",
        "\n",
        "        u, u_x = self.get_uv_and_derivatives(x)\n",
        "\n",
        "        grad_outputs = torch.ones_like(u_x, device=device) # tương tự, hỏi thầy ?????\n",
        "\n",
        "        # Tính đạo hàm bậc 2\n",
        "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "        #v_xx = torch.autograd.grad(v_x, x, grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "\n",
        "        # Tính f_u, f_v theo công thức PDE, ở đây có 2 pt residual theo 2 biến thì có 2 phần\n",
        "        # f_u =\n",
        "        # f_v =\n",
        "\n",
        "        return f_u, f_v\n",
        "\n",
        "############### Tính toán tổng loss ###############\n",
        "    def compute_loss(self):\n",
        "\n",
        "        # 1. Loss từ PDE residual (f)\n",
        "        f_u_pred, f_v_pred = self.get_f_uv(self.x_f, self.t_f) # tính residual\n",
        "        loss_f = torch.mean(f_u_pred**2) + torch.mean(f_v_pred**2) # Hàm Loss dạng MSE\n",
        "\n",
        "        # 2. Loss từ điều kiện ban đầu (t=0)\n",
        "        u0_pred, v0_pred, _, _, _, _ = self.get_uv_and_derivatives(self.x0, self.t0)\n",
        "        loss_0 = torch.mean((self.u0 - u0_pred)**2) + torch.mean((self.v0 - v0_pred)**2)\n",
        "\n",
        "        # 3. Loss từ điều kiện biên (lb, ub)\n",
        "        x_lb_with_grad = self.x_lb.clone().requires_grad_(True) # Báo cho PyTorch theo dõi đạo hàm của x_lb và x_ub\n",
        "        x_ub_with_grad = self.x_ub.clone().requires_grad_(True)\n",
        "\n",
        "        u_lb, v_lb, _, _, u_x_lb, v_x_lb = self.get_uv_and_derivatives(x_lb_with_grad, self.t_b)\n",
        "        u_ub, v_ub, _, _, u_x_ub, v_x_ub = self.get_uv_and_derivatives(x_ub_with_grad, self.t_b)\n",
        "\n",
        "        loss_b = torch.mean((u_lb - u_ub)**2) + \\\n",
        "                 torch.mean((v_lb - v_ub)**2) + \\\n",
        "                 torch.mean((u_x_lb - u_x_ub)**2) + \\\n",
        "                 torch.mean((v_x_lb - v_x_ub)**2)\n",
        "\n",
        "        # Tổng loss\n",
        "        return loss_0 + loss_b + loss_f\n",
        "\n",
        "############### Vòng huấn luyện chính ###############\n",
        "    def train_model(self, nIter):\n",
        "        # Dùng Adam optimizer\n",
        "        optimizer = torch.optim.Adam(self.parameters())\n",
        "\n",
        "        start_time = time.time() # bắt đầu set time hiện tại là 0\n",
        "        for it in range(nIter):\n",
        "            # BƯỚC 1: Xóa đạo hàm cũ\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # BƯỚC 2: Tính loss\n",
        "            loss = self.compute_loss()\n",
        "\n",
        "            # BƯỚC 3: Tính đạo hàm của loss theo các trọng số (backpropagation)\n",
        "            loss.backward()\n",
        "\n",
        "            # BƯỚC 4: Cập nhật trọng số\n",
        "            optimizer.step()\n",
        "            # các tham số (weights, bias) của mô hình đã được cập nhật theo đúng quy tắc của Adam Optimizer.\n",
        "\n",
        "            # In ra thông tin\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f'It: {it}, Loss: {loss.item():.3e}, Time: {elapsed:.2f}')\n",
        "                start_time = time.time()\n",
        "\n",
        "    def predict(self, X_star):\n",
        "        # Dự đoán\n",
        "        # Chuyển numpy array sang tensor\n",
        "        X_star_torch = torch.tensor(X_star, dtype=torch.float32).to(device)\n",
        "        x_tf = X_star_torch[:, 0:1].clone().requires_grad_(True)\n",
        "        t_tf = X_star_torch[:, 1:2].clone().requires_grad_(True)\n",
        "\n",
        "        # Chuyển sang chế độ đánh giá (tắt dropout, v.v.)\n",
        "        self.eval()\n",
        "\n",
        "        # Lấy dự đoán u, v\n",
        "        uv_star_torch = self(torch.cat([x_tf, t_tf], dim=1))\n",
        "        u_star = uv_star_torch[:, 0:1]\n",
        "        v_star = uv_star_torch[:, 1:2]\n",
        "\n",
        "        # Lấy dự đoán f_u, f_v\n",
        "        f_u_star, f_v_star = self.get_f_uv(x_tf, t_tf)\n",
        "\n",
        "        # Chuyển về chế độ huấn luyện\n",
        "        self.train()\n",
        "\n",
        "        # Chuyển kết quả về lại numpy (chuyển về CPU, tách khỏi đồ thị, rồi convert)\n",
        "        return u_star.cpu().detach().numpy(), \\\n",
        "               v_star.cpu().detach().numpy(), \\\n",
        "               f_u_star.cpu().detach().numpy(), \\\n",
        "               f_v_star.cpu().detach().numpy()\n"
      ],
      "metadata": {
        "id": "2K3_lpVeraLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Huấn luyện mô hình\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    # Forward\n",
        "    y_pred = model(x_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f'Epoch {epoch+1}: loss={loss.item():.4f}')\n",
        "\n",
        "# 5. Dự đoán với dữ liệu mới\n",
        "x_test = torch.tensor([[6.0]])\n",
        "y_test = model(x_test)\n",
        "print(\"Dự đoán y khi x=6:\", y_test.item())"
      ],
      "metadata": {
        "id": "_5QeBc7XrqSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}